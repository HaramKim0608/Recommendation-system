{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBD2WGNy401z"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon May 21 14:30:52 2018\n",
        "\n",
        "@author: jbk48\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Vk37oJy15D9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bi_LSTM():\n",
        "    \n",
        "    def __init__(self, lstm_units, num_class, keep_prob):\n",
        "        \n",
        "        self.lstm_units = lstm_units\n",
        "        \n",
        "        with tf.variable_scope('forward', reuse = tf.AUTO_REUSE):\n",
        "            \n",
        "            self.lstm_fw_cell = tf.nn.rnn_cell.LSTMCell(lstm_units, forget_bias=1.0, state_is_tuple=True)\n",
        "            self.lstm_fw_cell = tf.contrib.rnn.DropoutWrapper(self.lstm_fw_cell, output_keep_prob = keep_prob)\n",
        "            \n",
        "        with tf.variable_scope('backward', reuse = tf.AUTO_REUSE):\n",
        "            \n",
        "            self.lstm_bw_cell = tf.nn.rnn_cell.LSTMCell(lstm_units, forget_bias=1.0, state_is_tuple=True)\n",
        "            self.lstm_bw_cell = tf.contrib.rnn.DropoutWrapper(self.lstm_fw_cell, output_keep_prob = keep_prob)\n",
        "        \n",
        "        with tf.variable_scope('Weights', reuse = tf.AUTO_REUSE):\n",
        "           \n",
        "            self.W = tf.get_variable(name=\"W\", shape=[2 * lstm_units, num_class],\n",
        "                                dtype=tf.float32, initializer = tf.contrib.layers.xavier_initializer())\n",
        "            self.b = tf.get_variable(name=\"b\", shape=[num_class], dtype=tf.float32,\n",
        "                                initializer=tf.zeros_initializer())\n",
        "            \n",
        "            \n",
        "    def logits(self, X, W, b, seq_len):\n",
        "        \n",
        "        (output_fw, output_bw), states = tf.nn.bidirectional_dynamic_rnn(self.lstm_fw_cell, self.lstm_bw_cell,dtype=tf.float32,\n",
        "                                                                            inputs = X, sequence_length = seq_len)\n",
        "        ## concat fw, bw final states\n",
        "        outputs = tf.concat([states[0][1], states[1][1]], axis=1)\n",
        "        pred = tf.matmul(outputs, W) + b        \n",
        "        return pred\n",
        "        \n",
        "    def model_build(self, logits, labels, learning_rate = 0.001):\n",
        "        \n",
        "        with tf.variable_scope(\"loss\"):\n",
        "            \n",
        "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits , labels = labels)) # Softmax loss\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss) # Adam Optimizer\n",
        "            \n",
        "        return loss, optimizer\n",
        "    \n",
        "    def graph_build(self, avg_loss, avg_acc):\n",
        "        \n",
        "        tf.summary.scalar('Loss', avg_loss)\n",
        "        tf.summary.scalar('Accuracy', avg_acc)\n",
        "        merged = tf.summary.merge_all()\n",
        "        return merged"
      ],
      "metadata": {
        "id": "iaDJmknF5D_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}